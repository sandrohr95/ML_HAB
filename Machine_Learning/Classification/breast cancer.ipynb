{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93147423",
   "metadata": {},
   "source": [
    "# Predicción de Cancer de mama mediante Random Forest, PCA and SVM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9ef40",
   "metadata": {},
   "source": [
    "## Clase 1\n",
    "Exponer explicación del trabajo\n",
    "\n",
    "Dataset: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "\n",
    "La idea de la primera clase es aprender a realizar una exploración exhaustiva de los datos. Posteriormente se aprenderá a aplicar técnicas de selección de características. De evaluará el/los modelos aplicando y sin aplicar técnicas de Feature Selection y compararemos resultados.\n",
    "\n",
    "## Práctica Machine Learning.\n",
    "\n",
    "La idea es que sigan todos los pasos necesarios para crear un flujo de trabajo para utilizar algoritmos de Machine Learning:\n",
    "\n",
    "1º Lo primero será la lectura y exploración tal y cómo se hizo en clase. Además, inetntar investigar nuevas técnicas/métodos de visualización de datos para extraer información para su posterior análisis (la exploración  de los datos debe de estar correctamente comentada y fundamentada)\n",
    "\n",
    "2º Menejo de técnicas de pre-procesamiento de datos:\n",
    " - Evitar valores missing en el dataset\n",
    " - Estandarización/Normalización de los datos siempre y cuando sea necesario (se puede comparar los resultados aplicando la estandarización de los datos y sin aplicar)\n",
    " - Feature Selection. Aplicar algún/os método/s de selección de variables. La idea es entrenar el modelo con todas las características del dataset y evaluarlo, y después aplicar un método de selección de variables y volver a entrenar el modelo con las características seleccionadas y comparar resultados. Si se aplica más de un método de selección de variables se pueden comparar los resultados.\n",
    "NOTA: para fijar el número de características elegidas por el método habrá que justificar el porqué, es decir, habrá que escoger el número óptimo de características para elegir el mejor modelo.\n",
    " - Dominio del Método PCA (Principal Component Analysis). Esta vez aplicaremos un PCA sobre el dataset completo y compararemos los resultados obtenidos.\n",
    "\n",
    "3º Selección del modelo:\n",
    "- Debemos de evaluar distintos modelos de clasificación y comparar sus resultados\n",
    "\n",
    "4º Manejo de técnicas de hiperparametrización. Tenemos que saber aplicar al menos parametrización manual e hiperparametrización automática con el método GridSearch. Para definir el espacio de búsqueda de parámetros primero debemos de hacer un estudio previo del funcionamiento del algoritmo y de sus parámetros de entrada.\n",
    "\n",
    "5º Evaluación del modelo. Debemos de conocer y saber aplicar distintas métricas para evaluar los modelos.\n",
    "\n",
    "5.1º Además, se aplicará Cross-Validación para evaluar el modelo utilizando distintos fragmentos para entrenar y evaluar el modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98461707",
   "metadata": {},
   "source": [
    "## Preparar entorno de trabajo\n",
    "\n",
    "### Crear entorno virtual (Conda/Python env)\n",
    "\n",
    "Una vez creado activamos el entorno con 'conda activate myentorno' para instalar la librerías necesarias\n",
    "\n",
    "### Instalamos las librerías necesarias\n",
    "\n",
    "Para instalar las librerías podemos hacerlo mediante \"pip install\" o \"conda install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43646621",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27227/1466159966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83836c6c",
   "metadata": {},
   "source": [
    "## Paso 1: Adquisición de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d179eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observación de todos los elementos del dataset\n",
    "# display(breast_cancer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb9ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elementos del dataset\n",
    "breast_cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec3571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(breast_cancer.DESCR[27:3130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977df182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observación de variables target\n",
    "# print(breast_cancer.data)\n",
    "# print(breast_cancer.target)\n",
    "print(breast_cancer.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee68f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(breast_cancer.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437903e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(breast_cancer.filename)\n",
    "print(breast_cancer.data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659a6fd3",
   "metadata": {},
   "source": [
    "### Lectura de los datos\n",
    "Crearemos un dataframe y lo rellenaremos con cada una de las \"features\" del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargamos los datos en un dataframe \n",
    "df_features = pd.DataFrame(breast_cancer.data, columns = breast_cancer.feature_names)\n",
    "# Obtenemos información general del dataset\n",
    "# df_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a647149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67317d4a",
   "metadata": {},
   "source": [
    "### Lectura de la variable \"target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f6d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - Benigno\n",
    "# 1 - Maligno\n",
    "df_target = pd.DataFrame(breast_cancer.target, columns=['target'])\n",
    "# df_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f5c3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b399fe0",
   "metadata": {},
   "source": [
    "Según la descripción del conjunto de datos, la distribución de la variable objetivo es: 212 - Maligno, 357 - Benigno. Por lo tanto, \"benign\" y \"maglinant\" se presentan como 1 y 0, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dfb71",
   "metadata": {},
   "source": [
    "Para empezar a trabajar concatenamos ambos dataframes: características (Features) y variable objetivo (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b65220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_features, df_target], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7f7a3",
   "metadata": {},
   "source": [
    "### Exploración de los datos\n",
    "\n",
    "Antes de comenzar con la exploración de los datos vamos a asignarle nombre a la variable \"target\". Por lo tanto, indicaremos que:\n",
    "- target = 1 (benigno) \n",
    "- target = 0 (maligno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb524335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podríamos añadir una nueva columna categórica como hicimos en el análisis de supervivencia\n",
    "# df.loc[data.target == 1, 'cancer'] = \"Malignant\"\n",
    "# df.loc[data.target == 0, 'cancer'] = \"Benign\"\n",
    "\n",
    "df['target'] = df['target'].apply(lambda x: \"Benign\"\n",
    "                                  if x == 1 else \"Malignant\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos información adicional sobre el conjunto de datos\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12d81c6",
   "metadata": {},
   "source": [
    "#### Distribución de la variable objetivo mediante histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"target\"].hist())\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d08199",
   "metadata": {},
   "source": [
    "Como ya hemos visto antes, la distribución de la variable objetivo es: \n",
    "- 212 - Maligno, \n",
    "- 357 - Benigno. \n",
    "\n",
    "Para visualizar la distribución de los datos mejor utilizaremos la librería seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827cdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_style nos permite cambiar los colores de nuestras gráficas (mirar la documentación)\n",
    "# sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(df['target'])\n",
    "plt.xlabel(\"Diagnosis\")\n",
    "plt.title(\"Distribución de la diagnosis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b43d3f",
   "metadata": {},
   "source": [
    "### Distribución de características\n",
    "\n",
    "Ahora echaré un vistazo a la distribución de cada característica y veré en qué se diferencian entre 'benigno' y 'maligno'. Para ver la distribución de múltiples variables, podemos usar el diagrama de violín, el diagrama de enjambre o el diagrama de caja. \n",
    "Vamos a probar cada una de estas técnicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce3afb1",
   "metadata": {},
   "source": [
    "#### ¡OJO! ESTANDARIZAR LOS DATOS\n",
    "Para visualizar distribuciones de múltiples características en una figura, primero necesito estandarizar los datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Estandarizamos solo las características (features) del dataset\n",
    "# Si tuviesemos que estandarizar también nuestra variable target se haría por separado\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_features)\n",
    "features_scaled = scaler.transform(df_features)\n",
    "\n",
    "# Concatenamos de nuevo nuestras características estandarizadas con la variable target que queremos predecir\n",
    "features_scaled = pd.DataFrame(data=features_scaled,\n",
    "                               columns=df_features.columns)\n",
    "\n",
    "df_scaled = pd.concat([features_scaled, df['target']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07b61",
   "metadata": {},
   "source": [
    "Ahora podemos observar que nuestros datos han sido escalados correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3852c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1244f6d4",
   "metadata": {},
   "source": [
    "Cabe destacar que ahora todas las columnas (features) estan comprendidas en un mismo rango de valoresc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee25990",
   "metadata": {},
   "source": [
    "Antes de visualizar vamos a crear un dataframe para que nos sea mas fácil el manejo de los datos para la visualización.\n",
    "\n",
    "Vamos a utilizar la función pandas.melt(). Esta función es útil para transformar un DataFrame en un formato en el que una o más columnas son variables de identificación (id_vars), mientras que todas las demás columnas, serán consideradas variables de características."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a218b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_melt = pd.melt(df_scaled, id_vars='target',\n",
    "                         var_name='features', value_name='value')\n",
    "df_scaled_melt.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f749457",
   "metadata": {},
   "source": [
    "Para observar la distribución de los datos vamos a crear tres visualizaciones distintas (aunque con una sería suficiente, el uso de más gráficas de distribución de variables nos puede proporcionar información adicional):\n",
    "- BOX PLOT:  es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. De esta manera, se muestran a simple vista la mediana y los cuartiles de los datos, y también pueden representarse sus valores atípicos. \n",
    "VEASE (https://es.wikipedia.org/wiki/Diagrama_de_caja) para la interpretación del diagrama de cajas\n",
    "- SWARM PLOT: Un diagrama de enjambre es otra forma de trazar la distribución de un atributo o la distribución conjunta de un par de atributos.\n",
    "- VIOLIN PLOT: Los diagramas de violín son similares a los diagramas de caja (box plot), excepto que también muestran la densidad de probabilidad de los datos en diferentes valores. Estos gráficos incluyen un marcador para la mediana de los datos y un cuadro que indica el rango intercuartílico, como en los gráficos de caja estándar. En este diagrama de caja se superpone una estimación de la densidad del núcleo. Al igual que los diagramas de caja, los diagramas de violín se utilizan para representar la comparación de una distribución variable (o distribución de muestra) entre diferentes \"categorías\".\n",
    "\n",
    "\n",
    "Como hay 30 características en nuestro dataset utlizaremos 10 para cada uno de los tipos de visualizaciones que hemos expuesto.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36badb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def violin_plot(features, name):\n",
    "    \"\"\"\n",
    "    This function creates violin plots of features given in the argument.\n",
    "    \"\"\"\n",
    "    # Create query\n",
    "    query = ''\n",
    "    for x in features:\n",
    "        query += \"features == '\" + str(x) + \"' or \"\n",
    "    query = query[0:-4]\n",
    "\n",
    "    # Create data for visualization\n",
    "    data = df_scaled_melt.query(query)\n",
    "\n",
    "    # Plot figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.violinplot(x='features',\n",
    "                   y='value',\n",
    "                   hue='target',\n",
    "                   data=data,\n",
    "                   split=True,\n",
    "                   inner=\"quart\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Características\")\n",
    "    plt.ylabel(\"Datos estandarizados\")\n",
    "\n",
    "\n",
    "def swarm_plot(features, name):\n",
    "    \"\"\"\n",
    "    This function creates swarm plots of features given in the argument.\n",
    "    \"\"\"\n",
    "    # Create query\n",
    "    query = ''\n",
    "    for x in features:\n",
    "        query += \"features == '\" + str(x) + \"' or \"\n",
    "    query = query[0:-4]\n",
    "\n",
    "    # Create data for visualization\n",
    "    data = df_scaled_melt.query(query)\n",
    "\n",
    "    # Plot figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.swarmplot(x='features', y='value', hue='target', data=data)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Características\")\n",
    "    plt.ylabel(\"Datos estandarizados\")\n",
    "\n",
    "\n",
    "def box_plot(features, name):\n",
    "    \"\"\"\n",
    "    This function creates box plots of features given in the argument.\n",
    "    \"\"\"\n",
    "    # Create query\n",
    "    query = ''\n",
    "    for x in features:\n",
    "        query += \"features == '\" + str(x) + \"' or \"\n",
    "    query = query[0:-4]\n",
    "\n",
    "    # Create data for visualization\n",
    "    data = df_scaled_melt.query(query)\n",
    "\n",
    "    # Plot figure\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='features', y='value', hue='target', data=data)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Características\")\n",
    "    plt.ylabel(\"Datos estandarizados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ec6a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la distribución de las diez primeras características\n",
    "box_plot(df.columns[0:10], \"Box Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25027ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "swarm_plot(df.columns[10:20], \"Swarm Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4a84a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "violin_plot(df.columns[20:30], \"Violin Plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be9fafc",
   "metadata": {},
   "source": [
    "De los gráficos anteriores, podemos extraer algunas ideas de los datos:\n",
    "\n",
    "- La mediana de algunas características es muy diferente entre \"malignas\" y \"benignas\". Esta separación se puede ver claramente en los diagramas de caja. Pueden ser muy buenas características para la clasificación. Por ejemplo:mean radius, mean area, mean concave points, worst radius, worst perimeter, worst area, worst concave points.\n",
    "- Sin embargo, hay distribuciones que parecen similares entre \"maligno\" y \"benigno\". Por ejemplo:  mean smoothness, mean symmetry, mean fractual dimension, smoothness error. Por lo tanto, estas características no nos proporcionarán mucha ayuda en la tarea de clasificación.\n",
    "- Otras características tienen distribuciones similares, por lo que pueden estar altamente correlacionadas entre sí. Por ejemplo: mean perimeter vs. mean area, mean concavity vs. mean concave points y  worst symmetry vs. worst fractal dimension. Por lo tanto, quizás no deberíamos incluir estas variables altamente correlacionadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe1e1a",
   "metadata": {},
   "source": [
    "###  Correlación entre variables\n",
    "\n",
    "Como se ha observado anteriormente, algunas variables en el conjunto de datos pueden estar altamente correlacionadas entre sí. Exploremos la correlación de los tres ejemplos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c8111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(var):\n",
    "    \"\"\"\n",
    "    1. Print correlation\n",
    "    2. Create jointplot\n",
    "    \"\"\"\n",
    "    # Print correlation\n",
    "    print(\"Correlation: \", df[[var[0], var[1]]].corr().iloc[1, 0])\n",
    "\n",
    "    # Create jointplot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.jointplot(df[(var[0])], df[(var[1])], kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eef843",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(['mean perimeter', 'mean area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7589b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(['mean concavity', 'mean concave points'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525402f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation(['worst symmetry', 'worst fractal dimension'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c86229a",
   "metadata": {},
   "source": [
    "Dos de los tres conjuntos de variables estan altamente correlados. Una correlación mayor al 90%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a28cc1",
   "metadata": {},
   "source": [
    "Para ver si existe mas correlaciones entre variables podemos hacer un estudio genérico de todas las variables mediante un mapa de calor que nos proporcionará la correlación de todas con todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4606d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos la matriz de correlaciones\n",
    "corr_mat = df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_mat, annot=True, fmt='.1f',\n",
    "            cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "\n",
    "# Podemos crear una máscara para visualizar la mirad del mapa de correlaciones.\n",
    "# mask = np.zeros_like(corr_mat, dtype=np.bool)\n",
    "# mask[np.triu_indices_from(mask, k=1)] = True\n",
    "\n",
    "# # Plot heatmap\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# sns.heatmap(corr_mat, annot=True, fmt='.1f',\n",
    "#             cmap='RdBu_r', vmin=-1, vmax=1,mask=mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c12ec7",
   "metadata": {},
   "source": [
    "En el mapa de calor, podemos ver que muchas variables en el conjunto de datos están altamente correlacionadas. \n",
    "\n",
    "Podemos poner un valor umbral para ver qué variables tienen una correlación mayor a un 70% por ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13530953",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(corr_mat[corr_mat > 0.5], annot=True, fmt='.1f'\n",
    "            ,cmap=sns.cubehelix_palette(200))\n",
    "\n",
    "# Si hemos declarado la mascara podemos visualizarlo con mask\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# sns.heatmap(corr_mat[corr_mat > 0.7], annot=True, fmt='.1f'\n",
    "#             ,cmap=sns.cubehelix_palette(200), mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb3356",
   "metadata": {},
   "source": [
    "Podemos observar que tenemos muchas variables correlacionadas entre sí. Por lo tanto, podremos aplicar algún algoritmo de selección de variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba5571",
   "metadata": {},
   "source": [
    "## Paso 2: Preprocesamiento de los datos\n",
    "\n",
    "Comprobamos si los datos necesitan ser pre-procesados. \n",
    "\n",
    "Ahora necesitaríamos estandarizar nuestros datos antes de comenzar a trabajar pero ya lo hemos hecho previamente para visualizar los datos correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c3f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa560532",
   "metadata": {},
   "source": [
    "¡POR SUERTE! \n",
    "Los datos han sido limpiados y pre-procesados previamente por lo que podemos saltarnos este paso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a773f733",
   "metadata": {},
   "source": [
    "### Feature Selection (Selección de variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce52fa",
   "metadata": {},
   "source": [
    "Podemos aplicar un algoritmo de selección de variables para quedarnos con las variables más significativas para nuestra clasificación. \n",
    "\n",
    "#### Correlation based feature selection (CFS)\n",
    "En este caso vamos a utilizar Univariate Feature Selection pero podríamos utilizar/probar otro método de selección de variables. Este es un tipo de selección de características basada en correlación (CFS). Es una técnica de selección de características que utiliza el enfoque de filtro. Esta técnica de selección de características no depende de un algoritmo ML que se aplicará a las características seleccionadas. A menudo, los atributos de las características de un conjunto de datos pueden estar muy correlacionados entre sí. Estas características que se correlacionan en gran medida con otras características brindan información redundante. La técnica encuentra la correlación entre características. Las características que están altamente correlacionadas con otras características son excluido por el CFS. De manera similar, las características que se interrelacionan en gran medida con la etiqueta de la clase se conservan y seleccionan.\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/modules/classes.html?highlight=feature%20select#module-sklearn.feature_selection\n",
    "\n",
    "Elijo 5 porque en el mapa de calor pude ver alrededor de 5 grupos de características que están altamente correlacionadas. Aunque esto es un factor muy personal.\n",
    "\n",
    "#### Recursive feature elimination (RFE)\n",
    "La eliminación de características recursivas (RFE) es un método de selección de características que utiliza el enfoque de envoltura dependiente del modelo que se ha implementado previamente. Con este método de selección de variables primero tendremos que entrenar nuestro modelo con todas las variables para ver la importancia de cada una de las variables sobre el modelo. RFE implica la construcción de un modelo ML con todas las características originales en el conjunto de datos y las características se clasifican de acuerdo con su importancia cuantitativa para reducir el error de modelado.  Cada subconjunto de características se califica con una puntuación de precisión. Los subconjuntos de características que tienen las puntuaciones más altas se eligen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ac44e",
   "metadata": {},
   "source": [
    "\"Los vínculos entre variables con puntuaciones iguales se romperán de una manera no especificada.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc892ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "# Definimos feature Selection K=5\n",
    "feature_selection = SelectKBest(chi2, k=5)\n",
    "# Fit Feature Selection\n",
    "feature_selection.fit(df_features, df_target) #Run score function on (X, y) and get the appropriate features.\n",
    "# Seleccionamos las características\n",
    "selected_features = df_features.columns[feature_selection.get_support()]\n",
    "print(\"Las características selecionadas son: \", list(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac63c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aea2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get a mask, or integer index, of the features selected.\n",
    "feature_selection.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de010e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce X to the selected features with .transform(X)\n",
    "X = pd.DataFrame(feature_selection.transform(df_features),\n",
    "                 columns=selected_features)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc120c20",
   "metadata": {},
   "source": [
    "Vamos a crear un diagrama de pares (PAIRPLOT) para ver cómo se diferencian estas características en 'maligno' y en 'benigno'.\n",
    "\n",
    "Esta gráfica también es conocida como scatterplot matrix\n",
    "\n",
    "Una gráfica de pares nos permite ver tanto la distribución de variables individuales como las relaciones entre dos variables. Los diagramas de pares son un gran método para identificar tendencias para el análisis de seguimiento.\n",
    "\n",
    "El diagrama de pares se basa en dos figuras básicas, el histograma y el diagrama de dispersión. El histograma en la diagonal nos permite ver la distribución de una sola variable, mientras que los diagramas de dispersión en los triángulos superior e inferior muestran la relación (o falta de ella) entre dos variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3583dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(pd.concat([X, df['target']], axis=1))\n",
    "# Le podemos indicar la variable target para visualizar la diferencia que existe entre Maligno y benigno \n",
    "# en cada una de las variables y en la relación entre ellas\n",
    "sns.pairplot(pd.concat([X, df['target']], axis=1), hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e26982b",
   "metadata": {},
   "source": [
    "### Ejercicio en clase\n",
    "Probar otros métodos de selección de características alojados en la librería de Scikit-Learn y comparar los resultados. ¿Son las misma características?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6269f985",
   "metadata": {},
   "source": [
    "## Paso 3: Preparar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f486ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estamos cogiendo los datos una vez aplicada la selección de variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_target['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099d94b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f200e",
   "metadata": {},
   "source": [
    "## Paso 4: Selección del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e26c1c",
   "metadata": {},
   "source": [
    "Estamos viendo un caso de uso de clasificación binaria [0:maligno, 1:benigno]. Por lo tanto, debemos de elegir algún algoritmo de clasificación para implementar nuestro modelo. \n",
    "\n",
    "Lo primero y más importante para elegir con éxito el algoritmo que más se adapta a nuestras necesidades es:\n",
    "\n",
    "- Determinar qué queremos conseguir\n",
    "- Ver qué datos disponemos\n",
    "\n",
    "Una vez que se tienen claros estos dos puntos y conociendo los algoritmos de machine learning existentes, podremos escoger el que mejor se adapte a nuestras necesidades. Sin embargo, suponiendo que no hemos trabajando nunca en un caso de uso similar probaremos una batería de algoritmos ML y los compararemos para ver cuál de ellos no ofrece mejores resultados en este caso.\n",
    "\n",
    "Algunos de los algoritmos ML más comunes para clasificación:\n",
    "\n",
    "- Decision Tree/Random Forest\n",
    "- K Nearest Neighbor\n",
    "- Naive Bayes\n",
    "- Support Vector Machine\n",
    "- Logistic Regression\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc114f",
   "metadata": {},
   "source": [
    "### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c151afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f9898",
   "metadata": {},
   "source": [
    "## Paso 5:  Evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea066cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0310859e",
   "metadata": {},
   "source": [
    "accuracy, recall, f1-score es aproximadamente un  97%. \n",
    "\n",
    "Se puede observar en la matriz de confusión que el modelo solo ha fallado en 6 predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab4c20",
   "metadata": {},
   "source": [
    "### Otro Método para feature selection\n",
    "Ahora podríamos utilizar todo el conjunto de datos para entrenar nuestro algoritmo de Random Forest y llamar a la función “feature_importances_” para obtener la importancia de variables para este modelo en concreto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd05295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos todos los datos para hacer el SPLIT train/test\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df_target['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features, y, test_size=0.33, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330a7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc60df8f",
   "metadata": {},
   "source": [
    "Primero evaluamos el modelo entrenado con todas las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107fedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)\n",
    "# Make predictions on test data\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5339ec",
   "metadata": {},
   "source": [
    "Podemos observar que entrenando el modelo con todas las variables obtenemos resultados bastante peores que si aplicamos selección de características. Por este motivo, vamos a probar como funciona otro método de Feature Selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addc4874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_features(model):\n",
    "    feats = {}\n",
    "    for feature, importance in zip(df_features.columns, model.feature_importances_):\n",
    "        feats[feature] = importance\n",
    "\n",
    "    importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-Importance'})\n",
    "    importances = importances.sort_values(by='Gini-Importance', ascending=False)\n",
    "    importances = importances.reset_index()\n",
    "    importances = importances.rename(columns={'index': 'Features'})\n",
    "#     display(importances)\n",
    "    return importances\n",
    "\n",
    "importances = get_important_features(rfc)\n",
    "\n",
    "def plot_important_features(importances):\n",
    "    sns.set(font_scale = 5)\n",
    "    sns.set(style=\"whitegrid\", color_codes=True, font_scale = 1.7)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(30,15)\n",
    "    sns.barplot(x=importances['Gini-Importance'], y=importances['Features'], data=importances, color='skyblue')\n",
    "    plt.xlabel('Importance', fontsize=25, weight = 'bold')\n",
    "    plt.ylabel('Features', fontsize=25, weight = 'bold')\n",
    "    plt.title('Feature Importance', fontsize=25, weight = 'bold')\n",
    "    display(plt.show())\n",
    "\n",
    "# Plot feature importance\n",
    "plot_important_features(importances)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3517c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most_important features\n",
    "importances = importances.drop(importances[importances['Gini-Importance'] < 0.08].index)\n",
    "# Plot most_important features\n",
    "plot_important_features(importances)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e8de07",
   "metadata": {},
   "source": [
    "Ahora vamos a entrenar el modelo solo con estas características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a01770",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = list(importances['Features'])\n",
    "print(important_features)\n",
    "\n",
    "# train_important = df_train[important_features]\n",
    "# test_important = df_test[important_features]\n",
    "# # Convert to numpy\n",
    "# X_train = np.array(train_important)\n",
    "# X_test = np.array(test_important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_target['target']\n",
    "X = df_features[important_features] #Escogemos solo las columnas mas representativas para el modelo\n",
    "print(X.head())\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42) \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "rfc = RandomForestClassifier(n_estimators=200)\n",
    "rfc.fit(X_train, y_train)\n",
    "# Make predictions on test data\n",
    "y_pred = rfc.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32158239",
   "metadata": {},
   "source": [
    "Hemos visto como podemos mejorar la precisión del modelo mediante un pre-procesamiento de los datos con Feature Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
